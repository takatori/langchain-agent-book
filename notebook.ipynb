{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f4d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent-book\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"LANGCHAIN_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3510d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='はじめまして。私はAIアシスタントのChatGPTです。人工知能の分野で研究開発されたシステムで、皆さまの様々な質問や要望にお答えすることができます。\\n私は人工知能ですが、人間のように感情を持っているわけではありません。ただ、皆さまとコミュニケーションを取りながら、できる限り丁寧で適切な回答を心がけています。\\nどうぞよろしくお願いいたします。' additional_kwargs={} response_metadata={'id': 'msg_015UZGfdZ7M98iRS66bc9nRS', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 18, 'output_tokens': 152, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-haiku-20240307'} id='run--f8fc316b-33ff-4f45-a323-286b643c3643-0' usage_metadata={'input_tokens': 18, 'output_tokens': 152, 'total_tokens': 170, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0.0)\n",
    "output = model.invoke(\"自己紹介をしてください。\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bfe1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はい、ジョンさんと言っていただきました。私はあなたの名前を覚えています。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"こんにちは！私はジョンといいます！\"),\n",
    "    AIMessage(content=\"こんにちは、ジョンさん！私はあなたのアシスタントです。どのようにお手伝いできますか？\"),\n",
    "    HumanMessage(content=\"私の名前がわかりますか？\"),\n",
    "]\n",
    "\n",
    "ai_message = model.invoke(messages)\n",
    "print(ai_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b6a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは!どうぞよろしくお願いいたします。何か質問やお手伝いできることはありますか?"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"こんにちは!\"),\n",
    "]\n",
    "\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c39ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='以下の料理のレシピを教えてください\\n\\n料理名: オムライス'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"以下の料理のレシピを教えてください\n",
    "                                      \n",
    "料理名: {dish}\"\"\")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"オムライス\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7311a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='ユーザが入力した料理のレシピを教えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='カレー', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザが入力した料理のレシピを教えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d36254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='こんにちは！私はジョンといいます！', additional_kwargs={}, response_metadata={}), AIMessage(content='こんにちは、ジョンさん！どのようにお手伝いできますか？', additional_kwargs={}, response_metadata={}), HumanMessage(content='私の名前がわかりますか？', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"こんにちは！私はジョンといいます！\"),\n",
    "            AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
    "        ],\n",
    "        \"input\": \"私の名前がわかりますか？\",\n",
    "    }\n",
    ")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474ca56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='ユーザーが入力した料理のレシピを考えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='オムライス', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"oshima/recipe\")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"オムライス\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99e00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c01d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cd38a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザが入力した料理のレシピを考えてください。\\n\\n{format_instructions}\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_with_format_instructions = prompt.partial(format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e330e5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== role: system ===\n",
      "ユーザが入力した料理のレシピを考えてください。\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
      "```\n",
      "=== role: human ===\n",
      "カレー\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt_with_format_instructions.invoke({\"dish\": \"カレー\"})\n",
    "print(\"=== role: system ===\")\n",
    "print(prompt_value.messages[0].content)\n",
    "print(\"=== role: human ===\")\n",
    "print(prompt_value.messages[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b0a1bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON instance for a curry recipe:\n",
      "\n",
      "{\n",
      "  \"ingredients\": [\n",
      "    \"Onion\",\n",
      "    \"Garlic\",\n",
      "    \"Ginger\",\n",
      "    \"Curry powder\",\n",
      "    \"Cumin\",\n",
      "    \"Coriander\",\n",
      "    \"Turmeric\",\n",
      "    \"Chicken or beef\",\n",
      "    \"Potatoes\",\n",
      "    \"Carrots\",\n",
      "    \"Coconut milk\",\n",
      "    \"Tomato paste\",\n",
      "    \"Salt\",\n",
      "    \"Pepper\"\n",
      "  ],\n",
      "  \"steps\": [\n",
      "    \"Chop the onion, garlic, and ginger.\",\n",
      "    \"Heat oil in a large pot and sauté the onion, garlic, and ginger until fragrant.\",\n",
      "    \"Add the curry powder, cumin, coriander, and turmeric. Stir and cook for 1-2 minutes.\",\n",
      "    \"Add the chicken or beef and brown it.\",\n",
      "    \"Add the potatoes and carrots. Pour in the coconut milk and tomato paste.\",\n",
      "    \"Season with salt and pepper to taste.\",\n",
      "    \"Simmer the curry for 30-45 minutes, until the meat and vegetables are tender.\",\n",
      "    \"Serve the curry over rice.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ai_message = model.invoke(prompt_value)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "340be671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Recipe'>\n",
      "ingredients=['Onion', 'Garlic', 'Ginger', 'Curry powder', 'Cumin', 'Coriander', 'Turmeric', 'Chicken or beef', 'Potatoes', 'Carrots', 'Coconut milk', 'Tomato paste', 'Salt', 'Pepper'] steps=['Chop the onion, garlic, and ginger.', 'Heat oil in a large pot and sauté the onion, garlic, and ginger until fragrant.', 'Add the curry powder, cumin, coriander, and turmeric. Stir and cook for 1-2 minutes.', 'Add the chicken or beef and brown it.', 'Add the potatoes and carrots. Pour in the coconut milk and tomato paste.', 'Season with salt and pepper to taste.', 'Simmer the curry for 30-45 minutes, until the meat and vegetables are tender.', 'Serve the curry over rice.']\n"
     ]
    }
   ],
   "source": [
    "recipe = output_parser.invoke(ai_message.content.strip(\"Here is a JSON instance for a curry recipe:\"))\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e68f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "こんにちは。私はAIアシスタントです。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "ai_message = AIMessage(content=\"こんにちは。私はAIアシスタントです。\")\n",
    "output = output_parser.invoke(ai_message)\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb4b660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オムライスのレシピは以下のようになります。\n",
      "\n",
      "【材料】\n",
      "- 卵 3個\n",
      "- 米 1カップ\n",
      "- 玉ねぎ 1/2個\n",
      "- ケチャップ 大さじ2\n",
      "- 塩・こしょう 適量\n",
      "\n",
      "【作り方】\n",
      "1. 玉ねぎを細かく切る。\n",
      "2. 卵を2つに分け、1つは溶き卵にする。もう1つは白身と黄身に分ける。\n",
      "3. 米を炒めて、玉ねぎ、ケチャップ、塩こしょうを加えて炒める。\n",
      "4. 炒めた米をオーブンシートなどに広げ、溶き卵を流し入れる。\n",
      "5. 卵が固まってきたら、白身と黄身に分けた卵を中央に置く。\n",
      "6. 包み込むように丸めて、皿に盛り付ける。\n",
      "7. 好みでケチャップをかけて完成。\n",
      "\n",
      "ポイントは、卵の固さと米の食感のバランスを取ることです。好みの具材を加えたり、ケチャップの量を調整するなど、アレンジも楽しめます。\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザが入力した料理のレシピを教えてください。\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "ai_message = chain.invoke({\"dish\": \"オムライス\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59748c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Recipe'>\n",
      "ingredients=['卵', '米', '玉ねぎ', 'ケチャップ', 'バター'] steps=['卵を溶いて、玉ねぎを炒める', '卵を流し入れて、ふわふわに仕上げる', 'ご飯を加えてよく混ぜる', 'ケチャップを加えて味付けする', 'オムレツの上にご飯を乗せる']\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザが入力した料理のレシピを教えてください。\\n\\n{format_instructions}\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "prompt_with_format_instructions = prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "chain = prompt_with_format_instructions | model | output_parser\n",
    "\n",
    "recipe = chain.invoke({\"dish\": \"オムライス\"})\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d58bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "raw_docs = loader.load()\n",
    "print(len(raw_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fe8dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 6803, which is longer than the specified 1000\n",
      "Created a chunk of size 3302, which is longer than the specified 1000\n",
      "Created a chunk of size 1851, which is longer than the specified 1000\n",
      "Created a chunk of size 1639, which is longer than the specified 1000\n",
      "Created a chunk of size 9269, which is longer than the specified 1000\n",
      "Created a chunk of size 2579, which is longer than the specified 1000\n",
      "Created a chunk of size 17715, which is longer than the specified 1000\n",
      "Created a chunk of size 1700, which is longer than the specified 1000\n",
      "Created a chunk of size 1135, which is longer than the specified 1000\n",
      "Created a chunk of size 1126, which is longer than the specified 1000\n",
      "Created a chunk of size 1098, which is longer than the specified 1000\n",
      "Created a chunk of size 1433, which is longer than the specified 1000\n",
      "Created a chunk of size 1300, which is longer than the specified 1000\n",
      "Created a chunk of size 1166, which is longer than the specified 1000\n",
      "Created a chunk of size 1351, which is longer than the specified 1000\n",
      "Created a chunk of size 1756, which is longer than the specified 1000\n",
      "Created a chunk of size 1643, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n",
      "Created a chunk of size 1313, which is longer than the specified 1000\n",
      "Created a chunk of size 1422, which is longer than the specified 1000\n",
      "Created a chunk of size 1662, which is longer than the specified 1000\n",
      "Created a chunk of size 1123, which is longer than the specified 1000\n",
      "Created a chunk of size 1174, which is longer than the specified 1000\n",
      "Created a chunk of size 1936, which is longer than the specified 1000\n",
      "Created a chunk of size 1441, which is longer than the specified 1000\n",
      "Created a chunk of size 1498, which is longer than the specified 1000\n",
      "Created a chunk of size 1046, which is longer than the specified 1000\n",
      "Created a chunk of size 1057, which is longer than the specified 1000\n",
      "Created a chunk of size 1363, which is longer than the specified 1000\n",
      "Created a chunk of size 1432, which is longer than the specified 1000\n",
      "Created a chunk of size 1662, which is longer than the specified 1000\n",
      "Created a chunk of size 1281, which is longer than the specified 1000\n",
      "Created a chunk of size 1752, which is longer than the specified 1000\n",
      "Created a chunk of size 1705, which is longer than the specified 1000\n",
      "Created a chunk of size 1266, which is longer than the specified 1000\n",
      "Created a chunk of size 1764, which is longer than the specified 1000\n",
      "Created a chunk of size 1378, which is longer than the specified 1000\n",
      "Created a chunk of size 1596, which is longer than the specified 1000\n",
      "Created a chunk of size 1063, which is longer than the specified 1000\n",
      "Created a chunk of size 1540, which is longer than the specified 1000\n",
      "Created a chunk of size 1888, which is longer than the specified 1000\n",
      "Created a chunk of size 1739, which is longer than the specified 1000\n",
      "Created a chunk of size 42090, which is longer than the specified 1000\n",
      "Created a chunk of size 1147, which is longer than the specified 1000\n",
      "Created a chunk of size 5835, which is longer than the specified 1000\n",
      "Created a chunk of size 1819, which is longer than the specified 1000\n",
      "Created a chunk of size 1182, which is longer than the specified 1000\n",
      "Created a chunk of size 3563, which is longer than the specified 1000\n",
      "Created a chunk of size 1053, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 5053, which is longer than the specified 1000\n",
      "Created a chunk of size 1006, which is longer than the specified 1000\n",
      "Created a chunk of size 1651, which is longer than the specified 1000\n",
      "Created a chunk of size 2096, which is longer than the specified 1000\n",
      "Created a chunk of size 8206, which is longer than the specified 1000\n",
      "Created a chunk of size 2933, which is longer than the specified 1000\n",
      "Created a chunk of size 2134, which is longer than the specified 1000\n",
      "Created a chunk of size 1915, which is longer than the specified 1000\n",
      "Created a chunk of size 1129, which is longer than the specified 1000\n",
      "Created a chunk of size 1021, which is longer than the specified 1000\n",
      "Created a chunk of size 1393, which is longer than the specified 1000\n",
      "Created a chunk of size 2357, which is longer than the specified 1000\n",
      "Created a chunk of size 2373, which is longer than the specified 1000\n",
      "Created a chunk of size 2989, which is longer than the specified 1000\n",
      "Created a chunk of size 2736, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 1048, which is longer than the specified 1000\n",
      "Created a chunk of size 1083, which is longer than the specified 1000\n",
      "Created a chunk of size 1014, which is longer than the specified 1000\n",
      "Created a chunk of size 1044, which is longer than the specified 1000\n",
      "Created a chunk of size 1012, which is longer than the specified 1000\n",
      "Created a chunk of size 1438, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n",
      "Created a chunk of size 1036, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1113, which is longer than the specified 1000\n",
      "Created a chunk of size 1337, which is longer than the specified 1000\n",
      "Created a chunk of size 1162, which is longer than the specified 1000\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 1158, which is longer than the specified 1000\n",
      "Created a chunk of size 3603, which is longer than the specified 1000\n",
      "Created a chunk of size 1330, which is longer than the specified 1000\n",
      "Created a chunk of size 1188, which is longer than the specified 1000\n",
      "Created a chunk of size 1170, which is longer than the specified 1000\n",
      "Created a chunk of size 1443, which is longer than the specified 1000\n",
      "Created a chunk of size 3138, which is longer than the specified 1000\n",
      "Created a chunk of size 1121, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "teext_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "docs = teext_splitter.split_documents(raw_docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45995579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takatorisatoshi/Desktop/playground/langchain-agent-book/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.030450331047177315, -0.008112591691315174, -0.013246072456240654, -0.062330130487680435, 0.03151698037981987, -0.045135218650102615, -0.002460327697917819, 0.09321574866771698, 0.05472954362630844, -0.022930661216378212]\n",
      "[0.04152926430106163, -0.004461626056581736, 0.006429820321500301, -0.04747125506401062, 0.03172549232840538, -0.04668398201465607, -0.0019512787694111466, 0.07106544822454453, 0.046124741435050964, -0.020641732960939407]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# モデルの名前を指定 (ローカルにダウンロードされる)\n",
    "# 日本語モデルの例: \"intfloat/multilingual-e5-large\" や \"sonoisa/t5-japanese-v1.1-base\" など\n",
    "embedding_model_name = \"intfloat/multilingual-e5-large\"\n",
    "model_kwargs = {'device': 'cpu'} # GPUがある場合は'cuda'を指定\n",
    "encode_kwargs = {'normalize_embeddings': False} # 必要に応じて正規化するかどうか\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "text = \"これはテスト用の文章です。\"\n",
    "embedding = hf_embeddings.embed_query(text)\n",
    "print(embedding[:10]) # 埋め込みの一部を表示\n",
    "\n",
    "texts = [\n",
    "    \"これは最初の文章です。\",\n",
    "    \"これは2番目の文章です。\"\n",
    "]\n",
    "embeddings = hf_embeddings.embed_documents(texts)\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93ef96ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=hf_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7674d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd567a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of context_docs: 4\n",
      "metadata = {'file_type': '.mdx', 'file_path': 'docs/docs/concepts/document_loaders.mdx', 'source': 'docs/docs/concepts/document_loaders.mdx', 'file_name': 'document_loaders.mdx'}\n",
      "# Document loaders\n",
      "<span data-heading-keywords=\"document loader,document loaders\"></span>\n",
      "\n",
      ":::info[Prerequisites]\n",
      "\n",
      "* [Document loaders API reference](/docs/how_to/#document-loaders)\n",
      ":::\n",
      "\n",
      "Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\n",
      "\n",
      "## Integrations\n",
      "\n",
      "You can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).\n",
      "\n",
      "## Interface\n",
      "\n",
      "Documents loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).\n",
      "\n",
      "Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.\n",
      "\n",
      "Here's a simple example:\n",
      "\n",
      "```python\n",
      "from langchain_community.document_loaders.csv_loader import CSVLoader\n"
     ]
    }
   ],
   "source": [
    "query = \"AWSのS3からデータを読み込むためのDocument loaderはありますか？\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(f\"length of context_docs: {len(context_docs)}\")\n",
    "\n",
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a89697b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はい、AWSのS3からデータを読み込むためのDocument loaderがあります。\n",
      "\n",
      "文脈から、以下の2つのDocument loaderが確認できます:\n",
      "\n",
      "1. S3DirectoryLoader\n",
      "2. S3FileLoader\n",
      "\n",
      "これらのDocument loaderを使用することで、S3のディレクトリやファイルからデータを読み込むことができます。\n",
      "\n",
      "具体的な使用例は以下のように記載されています:\n",
      "\n",
      "```python\n",
      "from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
      "```\n",
      "\n",
      "また、それぞれの使用例へのリンクも提供されています:\n",
      "\n",
      "- S3DirectoryLoader: `/docs/integrations/document_loaders/aws_s3_directory`\n",
      "- S3FileLoader: `/docs/integrations/document_loaders/aws_s3_file`\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈:\"\"\"\n",
    "{context}\n",
    "\"\"\"                                                                                    \n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")                  \n",
    "\n",
    "output = chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02878fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-agent-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
